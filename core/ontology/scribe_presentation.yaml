type: project
id: scribe_presentation
version: 1.0.0

metadata:
  name: "Scribe Fusion Presentation"
  category: research_presentation
  description: "Comprehensive visual presentation of Scribe Fusion Transformer with dynamic visualizations, batch validation, and complete research documentation"
  author: "Sovereignty Foundation"
  created: "2025-11-01"
  
purpose: |
  Present the Scribe Fusion Transformer's capabilities through:
  1. Real-time fusion visualization showing multimodal identity composition
  2. Statistical power analysis demonstrating semantic superiority over baselines
  3. Attention pattern analysis revealing learned fusion strategies
  4. Complete research documentation with theory, architecture, validation, and results

# Visual Language Definition
visual_language:
  color_palette:
    modalities:
      narrative: "#3b82f6"      # Blue - Story, memory, self-concept
      modal: "#10b981"           # Green - Expression, style, behavior
      temporal: "#f59e0b"        # Orange - Time, experience, intention
      role: "#8b5cf6"            # Purple - Context, relationships, identity
    
    semantic_states:
      coherent: "#22c55e"        # Success - High coherence
      drifting: "#eab308"        # Warning - Medium coherence
      incoherent: "#ef4444"      # Error - Low coherence
    
    structure:
      primary_bg: "linear-gradient(135deg, #1e293b 0%, #0f172a 100%)"
      card_bg: "rgba(255, 255, 255, 0.05)"
      border: "rgba(255, 255, 255, 0.1)"
      text_primary: "#f1f5f9"
      text_secondary: "rgba(241, 245, 249, 0.6)"
  
  typography:
    h1: { size: "28px", weight: 600, letter_spacing: "-0.5px" }
    h2: { size: "20px", weight: 600 }
    h3: { size: "16px", weight: 600 }
    body: { size: "14px", weight: 400 }
    caption: { size: "12px", weight: 400, opacity: 0.6 }
    monospace: { size: "13px", family: "'Fira Code', monospace" }
  
  animations:
    pulse: { duration: "2s", timing: "ease-in-out", loop: true }
    flow: { duration: "1.5s", timing: "linear", loop: true }
    glow: { duration: "3s", timing: "ease-in-out", loop: true }
    transition: { duration: "300ms", timing: "ease-out" }
    micro: { duration: "150ms", timing: "ease-out" }

# Ontological Dependencies
ontologies:
  - scribe_results  # Defines fusion_result, statistical_summary, batch_metadata

# Pulse Event Bindings
pulse_bindings:
  # FusionVisualization subscribes to sample results
  scribe.result.sample:
    target_component: FusionVisualization
    handler: onPulse
    data_mapping:
      fusion_result.coherence.mean -> coherence_score
      fusion_result.coherence.per_modality -> modality_coherences
      fusion_result.attention_weights -> attention_matrix
      fusion_result.semantic_drift -> drift_value
      fusion_result.latency_ms -> latency
  
  # StatisticalAnalysis subscribes to batch completion
  scribe.result.batch:
    target_component: StatisticalAnalysis
    handler: onPulse
    data_mapping:
      statistics.coherence -> coherence_distributions
      statistics.semantic_drift -> drift_distributions
      statistics.hypothesis_tests -> significance_tests
      statistics.by_category -> category_breakdown
  
  # Emit validation request
  scribe.validate.request:
    source_component: TestControls
    trigger: run_button_click
    payload_mapping:
      batch_size_select.value -> batch_size
      category_select.value -> category_filter

# UI Composition
composition:
  root: ScribePresentation
  children:
    - FusionVisualization      # Viewport 1 - Renders from scribe.result.sample
    - StatisticalAnalysis      # Viewport 2 - Renders from scribe.result.batch
    - ResearchReport           # Side Viewport - Static documentation

# Component Definitions
components:
  FusionVisualization:
    type: canvas_visualization
    layout: centered
    elements:
      - ModalityOrbs:
          count: 4
          modalities: [narrative, modal, temporal, role]
          size: 80px
          animation: pulse
          glow: true
      
      - AttentionFlows:
          type: bezier_paths
          source: modality_orbs
          target: central_wisp
          animation: flow
          thickness_scale: attention_weight
      
      - CentralWisp:
          size: 120px
          fill: gradient_blend_all_modalities
          animation: [formation, rotation, coherence_pulse]
          glow_intensity: coherence_score
      
      - MultiHeadDisplay:
          heads: 8
          layout: horizontal_row
          size: 40px
          color: dominant_modality
          scale: head_contribution
      
      - MetricsBar:
          position: bottom
          metrics: [coherence, latency, iteration]
          animation: count_up
  
  StatisticalAnalysis:
    type: chart_grid
    layout: "2x2"
    charts:
      - CoherenceDistribution:
          type: violin_plot
          x_axis: [Scribe, MiniLM]
          y_axis: coherence_score
          range: [0.0, 1.0]
          overlays: [mean, std, confidence_intervals]
          annotations: [p_value, effect_size]
      
      - SemanticDrift:
          type: line_chart
          x_axis: batch_sizes
          y_axis: mean_semantic_drift
          lines:
            - { name: Scribe, color: "#3b82f6", style: solid }
            - { name: MiniLM, color: "#6b7280", style: dashed }
          error_bars: confidence_95
      
      - AttentionHeatmap:
          type: heatmap
          rows: 8  # attention heads
          columns: 4  # modalities
          color_scale: white_to_modality_color
          interaction: hover_tooltip
      
      - CategoryPerformance:
          type: radar_chart
          axes: [personal_memory, creative_fiction, conversational, technical]
          metrics: coherence
          lines:
            - { name: Scribe, color: "#3b82f6", fill: true }
            - { name: MiniLM, color: "#6b7280", fill: false }
  
  ResearchReport:
    type: tabbed_document
    tabs:
      - Theory:
          sections:
            - title: "Multimodal Identity Fusion"
              content: |
                Human identity is not a single vector but a composition of four semantic dimensions 
                that must be fused while preserving their individual coherence.
                
                The Four Modalities:
                - Narrative (384-dim): Autobiographical memory, personal history, self-concept
                - Modal (384-dim): Communication style, emotional tone, behavioral patterns
                - Temporal (384-dim): Past experiences, present state, future intentions
                - Role (384-dim): Relationships, responsibilities, identity in community
            
            - title: "Coherence-Preserving Fusion"
              content: |
                Fusion must not destroy the semantic information in individual modalities.
                The fused Wisp should maintain high cosine similarity with each input embedding.
                
                Mathematical Definition:
                Coherence(Wisp, Modality_i) = cosine_similarity(Wisp, Modality_i)
                Target: Coherence >= 0.90 for all modalities
            
            - title: "Multi-Head Attention Architecture"
              content: |
                Different aspects of identity require different attention patterns.
                Multi-head attention learns to weight modalities contextually.
                
                Architecture:
                - 8 attention heads
                - Each head learns different fusion strategies
                - Heads specialize in different semantic relationships
                - Final Wisp is weighted combination of all heads
            
            - title: "Semantic Power"
              content: |
                Scribe should demonstrate greater semantic expressiveness than baseline models
                when composing multimodal identity.
                
                Measurable Metrics:
                1. Coherence: How well the Wisp preserves input semantics
                2. Distinctiveness: How well Wisps separate different identities
                3. Compositionality: How well the Wisp represents the interaction of modalities
      
      - Architecture:
          sections:
            - title: "Model Specifications"
              content: |
                Scribe Fusion Transformer v1.0
                - Parameters: 9,465,216 (9.5M)
                - Input: 4 × 384-dimensional embeddings
                - Output: 1 × 384-dimensional fused Wisp
                - Architecture: Transformer-based with multi-head attention
                - Attention Heads: 8
                - Hidden Dimensions: 512
                - Dropout: 0.1
            
            - title: "Training Methodology"
              content: |
                Training Configuration:
                - Dataset: 18,884 samples across 4 categories
                - Epochs: 8
                - Batch Size: 32
                - Learning Rate: 1e-4
                - Optimizer: AdamW
                - Hardware: 5× RTX 5090 GPUs
                
                Loss Function:
                Loss = α × MSE(Wisp, Target) + β × (1 - mean(Coherence))
            
            - title: "Dataset Composition"
              content: |
                Training Data Categories:
                - Personal Memory: 4,721 samples
                - Creative Fiction: 4,721 samples
                - Conversational Dialogue: 4,721 samples
                - Technical Documentation: 4,721 samples
                
                Total: 18,884 samples
                Each sample includes: narrative, modal, temporal, role embeddings + target Wisp
      
      - Validation:
          sections:
            - title: "Test Design"
              content: |
                Batch Validation Protocol:
                - Batch Sizes: 10, 50, 100, 200 samples
                - Categories: All 4 (personal_memory, creative_fiction, conversational, technical)
                - Baseline: MiniLM encoding of concatenated text
                - Metrics: Coherence, semantic drift, latency, attention patterns
            
            - title: "Metrics Definitions"
              content: |
                Primary Metrics:
                1. Coherence: cosine_similarity(Wisp, Modality_i) for each modality
                2. Semantic Drift: L2_distance(Wisp, Narrative_embedding)
                3. Latency: Time to compose Wisp (milliseconds)
                4. Attention Weights: Per-head, per-modality attention distribution
                
                Success Criteria:
                - Mean Coherence >= 0.90
                - Semantic Drift < 0.1
                - Latency < 100ms
                - Statistical significance: p < 0.05, Cohen's d > 0.5
            
            - title: "Statistical Methods"
              content: |
                Analysis Techniques:
                - Descriptive Statistics: Mean, standard deviation, confidence intervals
                - Hypothesis Testing: Two-sample t-tests (Scribe vs MiniLM)
                - Effect Size: Cohen's d for practical significance
                - Visualization: Violin plots, line charts, heatmaps, radar charts
      
      - Results:
          sections:
            - title: "Batch Test Results"
              content: "Dynamic content populated from test execution"
              dynamic: true
              data_source: batch_test_results
            
            - title: "Statistical Analysis"
              content: "Dynamic content with statistical comparisons"
              dynamic: true
              data_source: statistical_summary
            
            - title: "Attention Pattern Analysis"
              content: "Dynamic content showing attention specialization"
              dynamic: true
              data_source: attention_analysis
            
            - title: "Conclusions"
              content: |
                Summary of findings:
                - Scribe demonstrates superior coherence preservation
                - Multi-head attention learns meaningful specialization
                - Semantic power validated across all categories
                - Production-ready for Logos authentication system

# Events and Data Flow
events:
  # User Actions
  - name: batch_test.start
    source: user
    target: scribe_validation_service
    payload:
      batch_size: integer
      category: string | null
  
  - name: batch_test.sample_complete
    source: scribe_validation_service
    target: mirror
    payload:
      sample_id: string
      coherence: float
      semantic_drift: float
      latency_ms: float
      attention_weights: array[8][4]
  
  - name: batch_test.complete
    source: scribe_validation_service
    target: mirror
    payload:
      total_samples: integer
      mean_coherence: float
      std_coherence: float
      mean_drift: float
      mean_latency: float
      p_value: float
      effect_size: float
      attention_patterns: object
  
  # Visualization Updates
  - name: fusion.animate
    source: mirror
    target: fusion_visualization
    payload:
      modality_embeddings: array[4][384]
      attention_weights: array[8][4]
      wisp: array[384]
      coherence: float
  
  - name: charts.update
    source: mirror
    target: statistical_analysis
    payload:
      batch_results: array
      statistical_summary: object

# Integration Points
integration:
  services:
    - scribe_listener:
        topic: scribe
        events: [scribe.compose, scribe.composed]
    
    - scribe_validation_service:
        topic: scribe_validation
        events: [scribe_validation.run_test, scribe_validation.test_result, scribe_validation.test_complete]
    
    - core:
        topic: core
        events: [ontology_request, ontology_response]
  
  data_sources:
    - batch_test_results: "Real-time results from validation service"
    - statistical_summary: "Aggregated statistics and comparisons"
    - attention_analysis: "Attention pattern extraction and visualization"

# Performance Requirements
performance:
  target_fps: 60
  max_latency: 100ms
  batch_throughput: ">10 Wisps/second"
  memory_limit: "2GB"

# Accessibility
accessibility:
  color_contrast: "WCAG AAA (7:1)"
  keyboard_navigation: true
  screen_reader: true
  aria_labels: true
  focus_indicators: true

# Export Capabilities
export:
  formats: [json, csv, pdf]
  includes: [test_results, statistical_analysis, attention_patterns, visualizations]
